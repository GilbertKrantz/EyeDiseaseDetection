{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10951558,"sourceType":"datasetVersion","datasetId":6812365}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport random\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision import transforms, datasets\nimport torchvision.models as models\nimport timm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, random_split, Subset, Dataset\nfrom sklearn.metrics import (\n    accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score\n)\nfrom sklearn.preprocessing import label_binarize\nfrom tqdm import tqdm\nimport gc\n\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:11.680667Z","iopub.execute_input":"2025-05-07T15:22:11.680991Z","iopub.status.idle":"2025-05-07T15:22:16.455808Z","shell.execute_reply.started":"2025-05-07T15:22:11.680969Z","shell.execute_reply":"2025-05-07T15:22:16.455137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modified FilteredImageDataset class with Pterygium filtering\nclass FilteredImageDataset(Dataset):\n    def __init__(self, dataset, excluded_classes=None):\n        \"\"\"\n        Create a filtered dataset that excludes specific classes.\n        \n        Args:\n            dataset: Original dataset (ImageFolder or similar)\n            excluded_classes: List of class names to exclude (e.g., [\"Pterygium\"])\n        \"\"\"\n        self.dataset = dataset\n        self.excluded_classes = excluded_classes or []\n        \n        # Get original class information\n        self.orig_classes = dataset.classes\n        self.orig_class_to_idx = dataset.class_to_idx\n        \n        # Create indices of samples to keep (excluding specified classes)\n        self.indices = []\n        for idx, (_, target) in enumerate(dataset.samples):\n            class_name = self.orig_classes[target]\n            if class_name not in self.excluded_classes:\n                self.indices.append(idx)\n        \n        # Create new class mapping without excluded classes\n        remaining_classes = [c for c in self.orig_classes if c not in self.excluded_classes]\n        self.classes = remaining_classes\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(remaining_classes)}\n        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n        \n        # Create a mapping from old indices to new indices\n        self.target_mapping = {}\n        for old_class, old_idx in self.orig_class_to_idx.items():\n            if old_class in self.class_to_idx:\n                self.target_mapping[old_idx] = self.class_to_idx[old_class]\n        \n        print(f\"Filtered out classes: {self.excluded_classes}\")\n        print(f\"Remaining classes: {self.classes}\")\n        print(f\"Original dataset size: {len(dataset)}, Filtered dataset size: {len(self.indices)}\")\n\n    def __getitem__(self, index):\n        \"\"\"Get item from the filtered dataset with remapped class labels.\"\"\"\n        orig_idx = self.indices[index]\n        img, old_target = self.dataset[orig_idx]\n        \n        # Remap target to new class index\n        new_target = self.target_mapping[old_target]\n        \n        return img, new_target\n\n    def __len__(self):\n        \"\"\"Return the number of samples in the filtered dataset.\"\"\"\n        return len(self.indices)\n    \n    # Allow transform to be updated\n    def set_transform(self, transform):\n        \"\"\"Update the transform for the dataset.\"\"\"\n        self.dataset.transform = transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.457202Z","iopub.execute_input":"2025-05-07T15:22:16.457579Z","iopub.status.idle":"2025-05-07T15:22:16.465134Z","shell.execute_reply.started":"2025-05-07T15:22:16.457560Z","shell.execute_reply":"2025-05-07T15:22:16.464502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Definition","metadata":{}},{"cell_type":"code","source":"# Early stopping class\nclass EarlyStopping:\n    def __init__(self, patience=5, delta=0):\n        self.patience = patience\n        self.delta = delta\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        score = -val_loss\n        \n        if self.best_score is None:\n            self.best_score = score\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.counter = 0\n\n# Model architecture functions\ndef _get_feature_blocks(model):\n    \"\"\"\n    Utility: locate the main feature blocks container in a timm model.\n    Returns a list-like module of blocks.\n    \"\"\"\n    for attr in ('features', 'blocks', 'layers', 'stem'):  # common container names\n        if hasattr(model, attr):\n            return getattr(model, attr)\n    # fallback: collect all children except classifier/head\n    return list(model.children())[:-1]\n\ndef _freeze_except_last_n(blocks, n):\n    total = len(blocks)\n    for idx, block in enumerate(blocks):\n        requires = (idx >= total - n)\n        for p in block.parameters():\n            p.requires_grad = requires\n\ndef get_model_mobilenetv4(num_classes, freeze_layers=True, device='cuda'):\n    model = timm.create_model('mobilenetv4_conv_medium.e500_r256_in1k', pretrained=True)\n    if freeze_layers:\n        blocks = _get_feature_blocks(model)\n        _freeze_except_last_n(blocks, 2)\n    # replace classifier\n    in_features = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(in_features, 512),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.4),\n        nn.Linear(512, num_classes)\n    )\n    return model.to(device)\n\ndef get_model_levit(num_classes, freeze_layers=True, device='cuda'):\n    model = timm.create_model('levit_128s.fb_dist_in1k', pretrained=True)\n    if freeze_layers:\n        blocks = _get_feature_blocks(model)\n        _freeze_except_last_n(blocks, 2)\n    # Attempt to extract in_features from model.head or classifier\n    head = getattr(model, 'head_dist', None) or getattr(model, 'classifier', None)\n    linear = getattr(head, 'linear')\n    in_features = 384\n    model.head = nn.Sequential(\n        nn.Linear(in_features, 512),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.4),\n        nn.Linear(512, num_classes)\n    )\n    model.head_dist = nn.Sequential(\n        nn.Linear(in_features, 512),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.4),\n        nn.Linear(512, num_classes)\n    )\n    return model.to(device)\n\ndef get_model_efficientvit(num_classes, freeze_layers=True, device='cuda'):\n    model = timm.create_model('efficientvit_m1.r224_in1k', pretrained=True)\n    if freeze_layers:\n        blocks = _get_feature_blocks(model)\n        _freeze_except_last_n(blocks, 2)\n    # handle different head naming\n    head = getattr(model, 'head', None)\n    print(head)\n    linear = getattr(head, 'linear')\n    in_features = 192\n    model.head.linear = nn.Sequential(\n        nn.Linear(in_features, 512),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.4),\n        nn.Linear(512, num_classes)\n    )\n    return model.to(device)\n    \ndef get_model_gernet(num_classes, freeze_layers=True, device='cuda'):\n    \"\"\"\n    Load and configure a GENet (General and Efficient Network) model with customizable classifier.\n    \n    Args:\n        num_classes: Number of output classes\n        freeze_layers: If True, freeze all but the last 2 blocks\n        device: Device to load the model on ('cuda' or 'cpu')\n        \n    Returns:\n        Configured GENet model\n    \"\"\"\n    model = timm.create_model('gernet_s.idstcv_in1k', pretrained=True)\n    \n    if freeze_layers:\n        # For GENet, we need to specifically handle its structure\n        # It typically has a 'stem' and 'stages' structure\n        if hasattr(model, 'stem') and hasattr(model, 'stages'):\n            # Freeze stem completely\n            for param in model.stem.parameters():\n                param.requires_grad = False\n                \n            # Freeze all stages except the last two\n            stages = list(model.stages.children())\n            total_stages = len(stages)\n            for i, stage in enumerate(stages):\n                requires_grad = (i >= total_stages - 2)\n                for param in stage.parameters():\n                    param.requires_grad = requires_grad\n        else:\n            # Fallback to generic approach\n            blocks = _get_feature_blocks(model)\n            _freeze_except_last_n(blocks, 2)\n    \n    # Replace classifier\n    in_features = model.head.fc.in_features\n    model.head.fc = nn.Sequential(\n        nn.Linear(in_features, 512),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.4),\n        nn.Linear(512, num_classes)\n    )\n    return model.to(device)\n\ndef get_model_regnetx(num_classes, freeze_layers=True, device='cuda'):\n    \"\"\"\n    Load and configure a RegNetX model with customizable classifier.\n    \n    Args:\n        num_classes: Number of output classes\n        freeze_layers: If True, freeze all but the last 2 blocks\n        device: Device to load the model on ('cuda' or 'cpu')\n        \n    Returns:\n        Configured RegNetX model\n    \"\"\"\n    model = timm.create_model('regnetx_008.tv2_in1k', pretrained=True)\n    \n    if freeze_layers:\n        # Looking at the error, we need to inspect the model structure carefully\n        # Print the model structure to understand it better in real use\n        # print(model)\n        \n        # Direct approach: check the model structure and freeze components individually\n        # First, freeze all parameters\n        for param in model.parameters():\n            param.requires_grad = False\n            \n        # Then unfreeze the last few layers manually based on RegNetX structure\n        # RegNetX typically has 'stem' + 'trunk' structure in timm\n        if hasattr(model, 'trunk'):\n            # Unfreeze final stages of the trunk\n            trunk_blocks = list(model.trunk.children())\n            # Unfreeze approximately last 25% of trunk blocks\n            unfreeze_from = max(0, int(len(trunk_blocks) * 0.75))\n            for i in range(unfreeze_from, len(trunk_blocks)):\n                for param in trunk_blocks[i].parameters():\n                    param.requires_grad = True\n        \n        # Always unfreeze the classifier/head for fine-tuning\n        for param in model.head.parameters():\n            param.requires_grad = True\n    \n    # Replace classifier\n    in_features = model.head.fc.in_features\n    model.head.fc = nn.Sequential(\n        nn.Linear(in_features, 512),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.4),\n        nn.Linear(512, num_classes)\n    )\n    return model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.465842Z","iopub.execute_input":"2025-05-07T15:22:16.466084Z","iopub.status.idle":"2025-05-07T15:22:16.492719Z","shell.execute_reply.started":"2025-05-07T15:22:16.466067Z","shell.execute_reply":"2025-05-07T15:22:16.491782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training function","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, early_stopping, epochs=15, use_ddp=False):\n    \"\"\"\n    Train the model and perform validation using multiple GPUs.\n    Supports both DataParallel (DP) and DistributedDataParallel (DDP) modes.\n    \n    Args:\n        model: Model to train\n        criterion: Loss function\n        optimizer: Optimizer for training\n        scheduler: Learning rate scheduler\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data\n        early_stopping: Early stopping handler\n        epochs: Maximum number of epochs to train\n        use_ddp: Whether to use DistributedDataParallel (True) or DataParallel (False)\n    \"\"\"\n    # Check available GPUs\n    num_gpus = torch.cuda.device_count()\n    if num_gpus < 2:\n        print(f\"Warning: Requested multi-GPU training but only {num_gpus} GPU(s) available. Continuing with available resources.\")\n    else:\n        print(f\"Using {num_gpus} GPUs for training\")\n    \n    # Setup device and model\n    if num_gpus >= 2:\n        if use_ddp:\n            # For DistributedDataParallel\n            import torch.distributed as dist\n            from torch.nn.parallel import DistributedDataParallel as DDP\n            \n            # Initialize process group\n            dist.init_process_group(backend='nccl')\n            local_rank = dist.get_rank()\n            torch.cuda.set_device(local_rank)\n            device = torch.device(f\"cuda:{local_rank}\")\n            \n            model = model.to(device)\n            model = DDP(model, device_ids=[local_rank])\n        else:\n            # For DataParallel (simpler to use)\n            device = torch.device(\"cuda:0\")\n            model = model.to(device)\n            model = torch.nn.DataParallel(model)\n    else:\n        # Single GPU\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        model = model.to(device)\n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    # Store validation predictions and labels for final evaluation\n    all_val_labels = []\n    all_val_preds = []\n    all_val_scores = []\n    \n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        epoch_train_acc = correct / total\n        train_losses.append(epoch_train_loss)\n        train_accs.append(epoch_train_acc)\n        \n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        all_labels = []\n        all_preds = []\n        all_scores = []\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                running_loss += loss.item() * inputs.size(0)\n                probs = F.softmax(outputs, dim=1)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                \n                all_labels.extend(labels.cpu().numpy().tolist())\n                all_preds.extend(predicted.cpu().numpy().tolist())\n                all_scores.append(probs.cpu().numpy())\n        \n        epoch_val_loss = running_loss / len(val_loader.dataset)\n        epoch_val_acc = correct / total\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n        \n        all_scores = np.vstack(all_scores) if all_scores else np.array([])\n        \n        # Store validation results for the final epoch\n        all_val_labels = all_labels\n        all_val_preds = all_preds\n        all_val_scores = all_scores\n        \n        # Update learning rate scheduler\n        scheduler.step(epoch_val_loss)\n        \n        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n        print(f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n        \n        # Check early stopping\n        early_stopping(epoch_val_loss)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered!\")\n            break\n            \n        # Free up memory\n        del all_labels, all_preds, all_scores\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    # Clean up DDP if used\n    if num_gpus >= 2 and use_ddp:\n        dist.destroy_process_group()\n    \n    return model, train_losses, val_losses, train_accs, val_accs, all_val_labels, all_val_preds, all_val_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.494736Z","iopub.execute_input":"2025-05-07T15:22:16.495264Z","iopub.status.idle":"2025-05-07T15:22:16.517964Z","shell.execute_reply.started":"2025-05-07T15:22:16.495245Z","shell.execute_reply":"2025-05-07T15:22:16.517204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation plotting functions","metadata":{}},{"cell_type":"code","source":"def plot_roc_curves(y_true, y_scores, class_names):\n    \"\"\"\n    Plot ROC curves for multi-class classification.\n    \n    Parameters:\n    - y_true: true labels\n    - y_scores: predicted probability scores from model\n    - class_names: list of class names\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    if torch.is_tensor(y_true):\n        y_true = y_true.cpu().numpy()\n    if torch.is_tensor(y_scores):\n        y_scores = y_scores.cpu().numpy()\n    \n    n_classes = len(class_names)\n    \n    # Binarize the labels for one-vs-rest ROC calculation\n    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n    \n    # Compute ROC curve and ROC area for each class\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    \n    plt.figure(figsize=(12, 8))\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        \n        plt.plot(fpr[i], tpr[i], lw=2,\n                 label=f'{class_names[i]} (area = {roc_auc[i]:.2f})')\n    \n    # Plot the diagonal (random classifier)\n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    \n    # Calculate and plot micro-average ROC curve\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_scores.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"], \n             label=f'Micro-average (area = {roc_auc[\"micro\"]:.2f})', \n             lw=2, linestyle=':', color='deeppink')\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    # Return the AUC values for reporting\n    return roc_auc\n\ndef plot_pr_curves(y_true, y_scores, class_names):\n    \"\"\"\n    Plot Precision-Recall curves for multi-class classification.\n    \n    Parameters:\n    - y_true: true labels\n    - y_scores: predicted probability scores from model\n    - class_names: list of class names\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    if torch.is_tensor(y_true):\n        y_true = y_true.cpu().numpy()\n    if torch.is_tensor(y_scores):\n        y_scores = y_scores.cpu().numpy()\n    \n    n_classes = len(class_names)\n    \n    # Binarize the labels\n    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n    \n    # Compute PR curve and average precision for each class\n    precision = {}\n    recall = {}\n    avg_precision = {}\n    \n    plt.figure(figsize=(12, 8))\n    \n    for i in range(n_classes):\n        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n        avg_precision[i] = average_precision_score(y_true_bin[:, i], y_scores[:, i])\n        \n        plt.plot(recall[i], precision[i], lw=2,\n                 label=f'{class_names[i]} (AP = {avg_precision[i]:.2f})')\n    \n    # Calculate and plot micro-average PR curve\n    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n        y_true_bin.ravel(), y_scores.ravel())\n    avg_precision[\"micro\"] = average_precision_score(y_true_bin.ravel(), y_scores.ravel())\n    \n    plt.plot(recall[\"micro\"], precision[\"micro\"],\n             label=f'Micro-average (AP = {avg_precision[\"micro\"]:.2f})',\n             lw=2, linestyle=':', color='deeppink')\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curves')\n    plt.legend(loc=\"best\")\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    # Return the average precision values for reporting\n    return avg_precision\n\ndef plot_accuracy_and_loss(train_losses, val_losses, train_accs, val_accs):\n    plt.figure(figsize=(12, 5))\n    # Accuracy curve\n    plt.subplot(1, 2, 1)\n    plt.plot(train_accs, label=\"Train Accuracy\")\n    plt.plot(val_accs, label=\"Validation Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy Curve\")\n    plt.legend()\n    plt.grid(True)\n    \n    # Loss curve\n    plt.subplot(1, 2, 2)\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curve\")\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    # Ensure we're working with numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Get unique values in both arrays\n    unique_values = np.unique(np.concatenate([y_true, y_pred]))\n    print(f\"Unique values in confusion matrix data: {unique_values}\")\n    \n    # Create the confusion matrix with explicit labels\n    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    plt.show()\n\ndef plot_per_class_accuracy(y_true, y_pred, class_names):\n    # Convert to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Get number of expected classes\n    num_classes = len(class_names)\n    \n    # Create the confusion matrix with explicit labels\n    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n    \n    # Calculate per-class accuracy\n    per_class_accuracy = np.zeros(num_classes)\n    for i in range(num_classes):\n        if i < cm.shape[0] and np.sum(cm[i, :]) > 0:\n            per_class_accuracy[i] = cm[i, i] / np.sum(cm[i, :])\n    \n    # Create the bar plot\n    plt.figure(figsize=(14, 7))\n    plt.bar(range(num_classes), per_class_accuracy, color=\"skyblue\")\n    plt.xticks(range(num_classes), class_names, rotation=45, ha='right')\n    plt.xlabel(\"Classes\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Per-Class Accuracy\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.518997Z","iopub.execute_input":"2025-05-07T15:22:16.519284Z","iopub.status.idle":"2025-05-07T15:22:16.546032Z","shell.execute_reply.started":"2025-05-07T15:22:16.519262Z","shell.execute_reply":"2025-05-07T15:22:16.545330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\n\ndef compute_classification_metrics(y_true, y_pred, y_scores, num_classes, class_names, model_name=\"\"):\n    \"\"\"\n    Compute comprehensive classification metrics including ROC AUC, PR AUC, and Cohen's Kappa.\n    \n    Parameters:\n    - y_true: true labels\n    - y_pred: predicted labels\n    - y_scores: predicted probability scores from model\n    - num_classes: number of classes\n    - class_names: list of class names\n    - model_name: name of the model (for display purposes)\n    \n    Returns:\n    - accuracy: overall accuracy score\n    - report_dict: classification report as dictionary\n    - roc_auc_dict: ROC AUC scores by class\n    - pr_auc_dict: PR AUC scores by class\n    - kappa: Cohen's Kappa score\n    \"\"\"\n    # Calculate accuracy\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f\"Overall Accuracy: {accuracy:.4f}\")\n    \n    # Calculate and display Cohen's Kappa\n    kappa = cohen_kappa_score(y_true, y_pred)\n    print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n    \n    # Print formatted classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names))\n    \n    # Calculate ROC curves and AUC for each class\n    print(\"\\nCalculating ROC curves...\")\n    roc_auc_dict = plot_roc_curves(y_true, y_scores, class_names)\n    \n    # Calculate PR curves and AUC for each class\n    print(\"\\nCalculating PR curves...\")\n    pr_auc_dict = plot_pr_curves(y_true, y_scores, class_names)\n    \n    # Return metrics for comparison\n    return accuracy, report, roc_auc_dict, pr_auc_dict, kappa\n\n# Also update evaluate_on_test_set to include kappa\ndef evaluate_on_test_set(model, test_loader, dataset):\n    \"\"\"Evaluate a trained model on test dataset\"\"\"\n    class_names = dataset.classes\n    num_classes = len(class_names)\n    \n    model.eval()\n    device = next(model.parameters()).device\n    \n    all_labels = []\n    all_preds = []\n    all_scores = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_scores.append(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n    \n    all_scores = np.vstack(all_scores)\n    \n    # Compute metrics including kappa\n    accuracy, report_dict, roc_auc_dict, pr_auc_dict, kappa = compute_classification_metrics(\n        all_labels, all_preds, all_scores, num_classes, class_names)\n    \n    # Build results dictionary with kappa\n    results = {\n        'accuracy': accuracy,\n        'report': report_dict,\n        'roc_auc': roc_auc_dict,\n        'pr_auc': pr_auc_dict,\n        'kappa': kappa\n    }\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.547383Z","iopub.execute_input":"2025-05-07T15:22:16.547662Z","iopub.status.idle":"2025-05-07T15:22:16.571142Z","shell.execute_reply.started":"2025-05-07T15:22:16.547638Z","shell.execute_reply":"2025-05-07T15:22:16.570633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main training function","metadata":{}},{"cell_type":"code","source":"# Update the model_train function to include kappa in the results\ndef model_train(model, train_loader, val_loader, dataset, epochs=20):\n    model_name = type(model).__name__\n    if hasattr(model, 'pretrained_cfg') and 'name' in model.pretrained_cfg:\n        model_name = model.pretrained_cfg['name']\n        \n    print(f\"\\n{'='*20} Training {model_name} {'='*20}\\n\")\n        \n    class_names = dataset.classes\n    num_classes = len(class_names)\n    learning_rate = 0.001\n    \n    try:\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n        early_stopping = EarlyStopping(patience=5)\n        \n        model, train_losses, val_losses, train_accs, val_accs, val_labels, val_preds, val_scores = train_model(\n            model, nn.CrossEntropyLoss(), optimizer, scheduler,\n            train_loader, val_loader, early_stopping, epochs=epochs, use_ddp=False\n        )\n        \n        print(f\"\\n{'='*20} Evaluation for {model_name} {'='*20}\\n\")\n        \n        # Plot training curves\n        plot_accuracy_and_loss(train_losses, val_losses, train_accs, val_accs)\n        \n        # Process validation predictions and labels\n        try:\n            plot_confusion_matrix(val_labels, val_preds, class_names)\n            plot_per_class_accuracy(val_labels, val_preds, class_names)\n            \n            # Get metrics from the updated function including kappa\n            accuracy, report_dict, roc_auc_dict, pr_auc_dict, kappa = compute_classification_metrics(\n                val_labels, val_preds, val_scores, num_classes, class_names, model_name)\n            \n            # Build a results dictionary including kappa\n            results = {\n                'accuracy': accuracy,\n                'report': report_dict,\n                'roc_auc': roc_auc_dict,\n                'pr_auc': pr_auc_dict,\n                'kappa': kappa\n            }\n            \n            return results\n        except Exception as viz_error:\n            print(f\"Error in visualization: {viz_error}\")\n            import traceback\n            traceback.print_exc()\n            return {'accuracy': None}\n            \n    except Exception as e:\n        print(f'Error occurred when training {model_name}: {e}')\n        import traceback\n        traceback.print_exc()\n        return {'accuracy': None}\n    finally:\n        # Clean up memory\n        if 'optimizer' in locals():\n            del optimizer\n        if 'scheduler' in locals():\n            del scheduler\n        if 'early_stopping' in locals():\n            del early_stopping\n        if 'train_losses' in locals():\n            del train_losses\n            del val_losses\n            del train_accs\n            del val_accs\n            del val_labels\n            del val_preds\n            del val_scores\n        \n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.571806Z","iopub.execute_input":"2025-05-07T15:22:16.572176Z","iopub.status.idle":"2025-05-07T15:22:16.594545Z","shell.execute_reply.started":"2025-05-07T15:22:16.572146Z","shell.execute_reply":"2025-05-07T15:22:16.593850Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparison Function","metadata":{}},{"cell_type":"code","source":"def compare_models(models, train_loader, val_loader, test_loader, dataset, epochs=20, names=None):\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n    \n    val_results = {}\n    test_results = {}\n    best_model_obj = None\n    best_accuracy = -1\n    best_model_name = \"\"\n    \n    # Summary dictionaries for metrics\n    val_roc_auc_summary = {}\n    test_roc_auc_summary = {}\n    val_pr_auc_summary = {}\n    test_pr_auc_summary = {}\n    val_kappa_summary = {}\n    test_kappa_summary = {}\n    \n    for i, (model, name) in enumerate(zip(models, names)):\n        print(f\"\\n\\n{'#'*30} Training {name} ({i+1}/{len(models)}) {'#'*30}\\n\")\n        model_results = model_train(model, train_loader, val_loader, dataset, epochs)\n        \n        # Extract accuracy from results\n        accuracy = model_results.get('accuracy')\n        val_results[name] = accuracy\n        \n        # Extract and store metrics\n        if 'roc_auc' in model_results and 'micro' in model_results['roc_auc']:\n            val_roc_auc_summary[name] = model_results['roc_auc']['micro']\n        else:\n            val_roc_auc_summary[name] = None\n            \n        if 'pr_auc' in model_results and 'micro' in model_results['pr_auc']:\n            val_pr_auc_summary[name] = model_results['pr_auc']['micro']\n        else:\n            val_pr_auc_summary[name] = None\n            \n        # Store kappa score\n        if 'kappa' in model_results:\n            val_kappa_summary[name] = model_results['kappa']\n        else:\n            val_kappa_summary[name] = None\n        \n        # Evaluate on test set\n        if accuracy is not None:\n            print(f\"\\n{'='*20} Testing {name} on Test Set {'='*20}\\n\")\n            test_model_results = evaluate_on_test_set(model, test_loader, dataset)\n            \n            # Extract accuracy from test results\n            test_accuracy = test_model_results.get('accuracy')\n            test_results[name] = test_accuracy\n            \n            # Extract and store test metrics\n            if 'roc_auc' in test_model_results and 'micro' in test_model_results['roc_auc']:\n                test_roc_auc_summary[name] = test_model_results['roc_auc']['micro']\n            else:\n                test_roc_auc_summary[name] = None\n                \n            if 'pr_auc' in test_model_results and 'micro' in test_model_results['pr_auc']:\n                test_pr_auc_summary[name] = test_model_results['pr_auc']['micro']\n            else:\n                test_pr_auc_summary[name] = None\n                \n            # Store test kappa score\n            if 'kappa' in test_model_results:\n                test_kappa_summary[name] = test_model_results['kappa']\n            else:\n                test_kappa_summary[name] = None\n            \n            # Track best model\n            if test_accuracy > best_accuracy:\n                best_accuracy = test_accuracy\n                best_model_obj = copy.deepcopy(model)\n                best_model_name = name\n    \n    # Print comprehensive comparison\n    print(\"\\n\\n\" + \"=\"*100)\n    print(\"COMPREHENSIVE MODEL COMPARISON\")\n    print(\"=\"*100)\n    print(f\"{'Model':<20}{'Val Acc':<10}{'Test Acc':<10}{'Val ROC AUC':<14}{'Test ROC AUC':<14}{'Val PR AUC':<14}{'Test PR AUC':<14}{'Val Kappa':<14}{'Test Kappa':<14}\")\n    print(\"-\"*100)\n    \n    for name in val_results.keys():\n        val_acc = val_results[name]\n        test_acc = test_results.get(name, None)\n        val_roc = val_roc_auc_summary.get(name, None)\n        test_roc = test_roc_auc_summary.get(name, None)\n        val_pr = val_pr_auc_summary.get(name, None)\n        test_pr = test_pr_auc_summary.get(name, None)\n        val_kappa = val_kappa_summary.get(name, None)\n        test_kappa = test_kappa_summary.get(name, None)\n        \n        # Format values for display\n        val_acc_str = f\"{val_acc:.4f}\" if val_acc is not None else \"Failed\"\n        test_acc_str = f\"{test_acc:.4f}\" if test_acc is not None else \"N/A\"\n        val_roc_str = f\"{val_roc:.4f}\" if val_roc is not None else \"N/A\"\n        test_roc_str = f\"{test_roc:.4f}\" if test_roc is not None else \"N/A\"\n        val_pr_str = f\"{val_pr:.4f}\" if val_pr is not None else \"N/A\"\n        test_pr_str = f\"{test_pr:.4f}\" if test_pr is not None else \"N/A\"\n        val_kappa_str = f\"{val_kappa:.4f}\" if val_kappa is not None else \"N/A\"\n        test_kappa_str = f\"{test_kappa:.4f}\" if test_kappa is not None else \"N/A\"\n            \n        print(f\"{name:<20}{val_acc_str:<10}{test_acc_str:<10}{val_roc_str:<14}{test_roc_str:<14}{val_pr_str:<14}{test_pr_str:<14}{val_kappa_str:<14}{test_kappa_str:<14}\")\n    \n    # Identify best model based on test metrics\n    if test_results:\n        # Best model by accuracy\n        best_acc_model = max(test_results.items(), key=lambda x: x[1] if x[1] is not None else -1)\n        print(f\"\\nBest model by accuracy: {best_acc_model[0]} (Test Accuracy: {best_acc_model[1]:.4f})\")\n        \n        # Best model by ROC AUC (if available)\n        if any(v is not None for v in test_roc_auc_summary.values()):\n            best_roc_model = max(\n                [(k, v) for k, v in test_roc_auc_summary.items() if v is not None], \n                key=lambda x: x[1] if x[1] is not None else -1\n            )\n            print(f\"Best model by ROC AUC: {best_roc_model[0]} (Test ROC AUC: {best_roc_model[1]:.4f})\")\n        \n        # Best model by PR AUC (if available)\n        if any(v is not None for v in test_pr_auc_summary.values()):\n            best_pr_model = max(\n                [(k, v) for k, v in test_pr_auc_summary.items() if v is not None], \n                key=lambda x: x[1] if x[1] is not None else -1\n            )\n            print(f\"Best model by PR AUC: {best_pr_model[0]} (Test PR AUC: {best_pr_model[1]:.4f})\")\n            \n        # Best model by Kappa (if available)\n        if any(v is not None for v in test_kappa_summary.values()):\n            best_kappa_model = max(\n                [(k, v) for k, v in test_kappa_summary.items() if v is not None], \n                key=lambda x: x[1] if x[1] is not None else -1\n            )\n            print(f\"Best model by Cohen's Kappa: {best_kappa_model[0]} (Test Kappa: {best_kappa_model[1]:.4f})\")\n        \n        # Save the best model (by accuracy)\n        if best_model_obj is not None:\n            try:\n                model_save_path = f\"best_model_{best_model_name.lower().replace(' ', '_')}.pth\"\n                torch.save(best_model_obj.state_dict(), model_save_path)\n                print(f\"Best model saved to {model_save_path}\")\n            except Exception as save_error:\n                print(f\"Error saving best model: {save_error}\")\n    else:\n        print(\"\\nNo models successfully completed testing.\")\n    \n    print(\"=\"*100)\n    \n    # Visualize comparison\n    try:\n        # Create bar charts comparing different metrics\n        plot_model_comparison(val_results, test_results, val_roc_auc_summary, \n                             test_roc_auc_summary, val_pr_auc_summary, test_pr_auc_summary,\n                             val_kappa_summary, test_kappa_summary)\n    except Exception as viz_error:\n        print(f\"Error in comparison visualization: {viz_error}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.595427Z","iopub.execute_input":"2025-05-07T15:22:16.595661Z","iopub.status.idle":"2025-05-07T15:22:16.620099Z","shell.execute_reply.started":"2025-05-07T15:22:16.595635Z","shell.execute_reply":"2025-05-07T15:22:16.619560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_model_comparison(val_acc, test_acc, val_roc, test_roc, val_pr, test_pr, val_kappa, test_kappa):\n    \"\"\"\n    Create visualizations to compare model performance across different metrics including Cohen's Kappa.\n    \"\"\"\n    # Get the list of model names (should be the same across all dictionaries)\n    models = list(val_acc.keys())\n    \n    # Create a figure with 4 subplots for Accuracy, ROC AUC, PR AUC, and Kappa\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 24))\n    \n    # Plot Accuracy\n    x = np.arange(len(models))\n    width = 0.35\n    \n    val_acc_values = [val_acc.get(model, None) for model in models]\n    test_acc_values = [test_acc.get(model, None) for model in models]\n    \n    # Replace None with NaN for plotting\n    val_acc_values = [v if v is not None else float('nan') for v in val_acc_values]\n    test_acc_values = [v if v is not None else float('nan') for v in test_acc_values]\n    \n    ax1.bar(x - width/2, val_acc_values, width, label='Validation', color='skyblue')\n    ax1.bar(x + width/2, test_acc_values, width, label='Test', color='salmon')\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('Model Accuracy Comparison')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(models, rotation=45, ha='right')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot ROC AUC\n    val_roc_values = [val_roc.get(model, None) for model in models]\n    test_roc_values = [test_roc.get(model, None) for model in models]\n    \n    # Replace None with NaN for plotting\n    val_roc_values = [v if v is not None else float('nan') for v in val_roc_values]\n    test_roc_values = [v if v is not None else float('nan') for v in test_roc_values]\n    \n    ax2.bar(x - width/2, val_roc_values, width, label='Validation', color='skyblue')\n    ax2.bar(x + width/2, test_roc_values, width, label='Test', color='salmon')\n    ax2.set_ylabel('ROC AUC')\n    ax2.set_title('Model ROC AUC Comparison')\n    ax2.set_xticks(x)\n    ax2.set_xticklabels(models, rotation=45, ha='right')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Plot PR AUC\n    val_pr_values = [val_pr.get(model, None) for model in models]\n    test_pr_values = [test_pr.get(model, None) for model in models]\n    \n    # Replace None with NaN for plotting\n    val_pr_values = [v if v is not None else float('nan') for v in val_pr_values]\n    test_pr_values = [v if v is not None else float('nan') for v in test_pr_values]\n    \n    ax3.bar(x - width/2, val_pr_values, width, label='Validation', color='skyblue')\n    ax3.bar(x + width/2, test_pr_values, width, label='Test', color='salmon')\n    ax3.set_ylabel('PR AUC')\n    ax3.set_title('Model PR AUC Comparison')\n    ax3.set_xticks(x)\n    ax3.set_xticklabels(models, rotation=45, ha='right')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Plot Kappa scores\n    val_kappa_values = [val_kappa.get(model, None) for model in models]\n    test_kappa_values = [test_kappa.get(model, None) for model in models]\n    \n    # Replace None with NaN for plotting\n    val_kappa_values = [v if v is not None else float('nan') for v in val_kappa_values]\n    test_kappa_values = [v if v is not None else float('nan') for v in test_kappa_values]\n    \n    ax4.bar(x - width/2, val_kappa_values, width, label='Validation', color='skyblue')\n    ax4.bar(x + width/2, test_kappa_values, width, label='Test', color='salmon')\n    ax4.set_ylabel(\"Cohen's Kappa\")\n    ax4.set_title(\"Model Cohen's Kappa Comparison\")\n    ax4.set_xticks(x)\n    ax4.set_xticklabels(models, rotation=45, ha='right')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Create a comprehensive heatmap for all metrics\n    try:\n        plot_metrics_heatmap(models, val_acc_values, test_acc_values, \n                            val_roc_values, test_roc_values,\n                            val_pr_values, test_pr_values,\n                            val_kappa_values, test_kappa_values)\n    except Exception as e:\n        print(f\"Error creating metrics heatmap: {e}\")\n\ndef plot_metrics_heatmap(models, val_acc, test_acc, val_roc, test_roc, val_pr, test_pr, val_kappa, test_kappa):\n    \"\"\"\n    Create a heatmap visualization of all metrics for easy comparison across models.\n    \"\"\"\n    # Prepare data for heatmap\n    metric_names = ['Val Acc', 'Test Acc', 'Val ROC', 'Test ROC', \n                   'Val PR', 'Test PR', 'Val Kappa', 'Test Kappa']\n    \n    data = np.array([\n        val_acc, test_acc, val_roc, test_roc, val_pr, test_pr, val_kappa, test_kappa\n    ])\n    \n    # Create the heatmap\n    plt.figure(figsize=(12, 8))\n    ax = sns.heatmap(data, annot=True, fmt=\".4f\", cmap=\"YlGnBu\", \n                    xticklabels=models, yticklabels=metric_names)\n    \n    plt.title(\"Comprehensive Model Performance Metrics\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:16.595427Z","iopub.execute_input":"2025-05-07T15:22:16.595661Z","iopub.status.idle":"2025-05-07T15:22:16.620099Z","shell.execute_reply.started":"2025-05-07T15:22:16.595635Z","shell.execute_reply":"2025-05-07T15:22:16.619560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Function","metadata":{}},{"cell_type":"code","source":"train_dir = '/kaggle/input/eye-disease-image-dataset/Augmented Dataset/Augmented Dataset'  # For training (pre-augmented data)\neval_dir = '/kaggle/input/eye-disease-image-dataset/Original Dataset/Original Dataset'  # For val and test\nepochs = 15\nclasses_to_exclude = [\"Pterygium\"]\nbatch_size = 32\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntry:\n    # Load datasets\n    print(f\"Loading training dataset from {train_dir}...\")\n    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n    \n    print(f\"Loading evaluation dataset from {eval_dir}...\")\n    eval_dataset = datasets.ImageFolder(root=eval_dir, transform=transform)\n    \n    # Print dataset information\n    print(f\"Training dataset loaded with {len(train_dataset)} images across {len(train_dataset.classes)} classes.\")\n    print(f\"Training classes: {train_dataset.classes}\")\n    \n    print(f\"Evaluation dataset loaded with {len(eval_dataset)} images across {len(eval_dataset.classes)} classes.\")\n    print(f\"Evaluation classes: {eval_dataset.classes}\")\n    \n    # Filter datasets if needed\n    excluded_classes = classes_to_exclude or []\n    if excluded_classes:\n        print(f\"Filtering out classes: {excluded_classes}\")\n        filtered_train_dataset = FilteredImageDataset(train_dataset, excluded_classes=excluded_classes)\n        filtered_eval_dataset = FilteredImageDataset(eval_dataset, excluded_classes=excluded_classes)\n    else:\n        filtered_train_dataset = train_dataset\n        filtered_eval_dataset = eval_dataset\n    \n    # Check if the filtered classes match between training and evaluation datasets\n    if set(filtered_train_dataset.classes) != set(filtered_eval_dataset.classes):\n        print(\"Warning: Class mismatch between filtered training and evaluation datasets!\")\n        print(f\"Filtered training classes: {filtered_train_dataset.classes}\")\n        print(f\"Filtered evaluation classes: {filtered_eval_dataset.classes}\")\n        \n        # Find common classes\n        common_classes = set(filtered_train_dataset.classes).intersection(set(filtered_eval_dataset.classes))\n        print(f\"Common classes: {common_classes}\")\n        \n        # Create additional filtering based on common classes\n        filtered_train_dataset = FilteredImageDataset(train_dataset, \n                                                   included_classes=common_classes)\n        filtered_eval_dataset = FilteredImageDataset(eval_dataset, \n                                                  included_classes=common_classes)\n    \n    # Split evaluation dataset into validation and test sets\n    eval_ratio = 0.7  # 70% validation, 30% test\n    eval_size = len(filtered_eval_dataset)\n    val_size = int(eval_ratio * eval_size)\n    test_size = eval_size - val_size\n    \n    val_dataset, test_dataset = random_split(filtered_eval_dataset, [val_size, test_size])\n    \n    print(f\"Training set size: {len(filtered_train_dataset)}\")\n    print(f\"Validation set size: {len(val_dataset)}\")\n    print(f\"Test set size: {len(test_dataset)}\")\n    \n    # Create data loaders\n    train_loader = DataLoader(filtered_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    # Get the number of classes (after filtering)\n    num_classes = len(filtered_train_dataset.classes)\n    print(f\"Number of classes after filtering: {num_classes}\")\n    print(f\"Classes after filtering: {filtered_train_dataset.classes}\")\n    \n    # Initialize models\n    print(\"Initializing models...\")\n    \n    # Initialize models with the updated number of classes\n    all_models = {\n        \"MobileNetV4\": get_model_mobilenetv4(num_classes, freeze_layers=True),\n        \"LeViT\": get_model_levit(num_classes, freeze_layers=True),\n        \"EfficientViT\": get_model_efficientvit(num_classes, freeze_layers=True),\n        \"GENet\": get_model_gernet(num_classes, freeze_layers=True),\n        \"RegNetX\": get_model_regnetx(num_classes, freeze_layers=True)\n    }\n    \n    models = list(all_models.values())\n    model_names = list(all_models.keys())\n    \n    # Train and compare models\n    print(\"Starting model training and comparison...\")\n    compare_models(models, train_loader, val_loader, test_loader, filtered_train_dataset, epochs=epochs, names=model_names)\n    \nexcept Exception as e:\n    print(f\"Error in eye disease classification pipeline: {e}\")\n    import traceback\n    traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:22:59.764465Z","iopub.execute_input":"2025-05-07T15:22:59.765257Z","execution_failed":"2025-05-07T15:22:59.047Z"}},"outputs":[],"execution_count":null}]}